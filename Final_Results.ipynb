{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUauj_HZJJus",
        "outputId": "6c991b0e-3213-4bbc-f2b8-6a626e4604ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Directories created!\n",
            "Now upload your 2 ZIP files manually...\n"
          ]
        }
      ],
      "source": [
        "# SETUP & MOUNT DRIVE\n",
        "\n",
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p /content/datasets\n",
        "\n",
        "print(\" Directories created!\")\n",
        "print(\"Now upload your 2 ZIP files manually...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nwEd2RofZtDn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  #\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai3lCfq8PQ-K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.multiprocessing.set_start_method(\"spawn\", force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IAp1FMyTkI-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def tqdm_colab(iterable, **kwargs):\n",
        "    return tqdm(iterable, leave=False, dynamic_ncols=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTqhssaDJxz8",
        "outputId": "21d9f122-b6b0-49df-c638-ac462c7469e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 ZIP files:\n",
            "  - Figshare_Dataset_Patient_Level_Split.zip\n",
            "  - Figshare_ImageLevel_Split_PNGs.zip\n",
            "\n",
            " Extracting Figshare_Dataset_Patient_Level_Split.zip\n",
            " Extracting Figshare_ImageLevel_Split_PNGs.zip\n",
            "\n",
            " Extraction complete!\n",
            "\n",
            " Dataset structure:\n",
            "total 4.0K\n",
            "drwxr-xr-x 5 root root 4.0K Jan 26 12:03 Figshare_Dataset\n",
            "total 4.0K\n",
            "drwxr-xr-x 5 root root 4.0K Jan 26 12:03 Figshare_ImageLevel\n"
          ]
        }
      ],
      "source": [
        "# EXTRACT DATASETS\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Find uploaded files\n",
        "uploaded_files = list(Path('/content').glob('*.zip'))\n",
        "print(f\"Found {len(uploaded_files)} ZIP files:\")\n",
        "for f in uploaded_files:\n",
        "    print(f\"  - {f.name}\")\n",
        "\n",
        "# Extract patient-level split\n",
        "patient_zip = [f for f in uploaded_files if 'Patient_Level' in f.name][0]\n",
        "print(f\"\\n Extracting {patient_zip.name}\")\n",
        "with zipfile.ZipFile(patient_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/datasets/patient_split')\n",
        "\n",
        "# Extract image-level split\n",
        "image_zip = [f for f in uploaded_files if 'ImageLevel' in f.name][0]\n",
        "print(f\" Extracting {image_zip.name}\")\n",
        "with zipfile.ZipFile(image_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/datasets/normal_split')\n",
        "\n",
        "print(\"\\n Extraction complete!\")\n",
        "\n",
        "# Verify structure\n",
        "print(\"\\n Dataset structure:\")\n",
        "!ls -lh /content/datasets/patient_split\n",
        "!ls -lh /content/datasets/normal_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA8wPvDWKDh8",
        "outputId": "7ccb49af-c394-4ecc-c694-f90d4b4bf88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/983.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h All libraries installed!\n"
          ]
        }
      ],
      "source": [
        "# INSTALL LIBRARIES\n",
        "\n",
        "!pip install -q timm torchmetrics pillow matplotlib seaborn scikit-learn pandas tqdm\n",
        "\n",
        "print(\" All libraries installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emaD4xo0KE6x",
        "outputId": "31c14174-7cd3-45ed-b30e-ad130c7f1b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Configuration:\n",
            "  Device: cuda\n",
            "  Batch Size: 32\n",
            "  Epochs: 10\n",
            "  Image Size: 224x224\n",
            "  Classes: ['glioma', 'meningioma', 'pituitary']\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS & CONFIG\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import timm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# CONFIGURATION\n",
        "class Config:\n",
        "    # Dataset paths (will auto-detect structure)\n",
        "    PATIENT_SPLIT_ROOT = '/content/datasets/patient_split'\n",
        "    NORMAL_SPLIT_ROOT = '/content/datasets/normal_split'\n",
        "\n",
        "    # Classes\n",
        "    CLASSES = ['glioma', 'meningioma', 'pituitary']\n",
        "    NUM_CLASSES = 3\n",
        "\n",
        "    # Training hyperparameters\n",
        "    IMG_SIZE = 224\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 10\n",
        "    LR = 1e-4\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "\n",
        "    # Device\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Experiment tracking\n",
        "    RESULTS_DIR = '/content/results'\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Create results directory\n",
        "Path(config.RESULTS_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "print(f\" Configuration:\")\n",
        "print(f\"  Device: {config.DEVICE}\")\n",
        "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {config.EPOCHS}\")\n",
        "print(f\"  Image Size: {config.IMG_SIZE}x{config.IMG_SIZE}\")\n",
        "print(f\"  Classes: {config.CLASSES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQtTqMA-NGy0",
        "outputId": "a43ab547-490b-4869-dcd9-6a74d3a0fc65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Testing dataset loading...\n",
            "Loading Training from: /content/datasets/normal_split/Figshare_ImageLevel/Training\n",
            "  glioma: 467 images\n",
            "  meningioma: 1021 images\n",
            "  pituitary: 656 images\n",
            "Total: 2144 images\n",
            "\n",
            "Loading Validation from: /content/datasets/normal_split/Figshare_ImageLevel/Validation\n",
            "  glioma: 131 images\n",
            "  meningioma: 207 images\n",
            "  pituitary: 121 images\n",
            "Total: 459 images\n",
            "\n",
            "Loading Testing from: /content/datasets/normal_split/Figshare_ImageLevel/Testing\n",
            "  glioma: 110 images\n",
            "  meningioma: 198 images\n",
            "  pituitary: 153 images\n",
            "Total: 461 images\n",
            "\n",
            " Normal split loaded successfully!\n",
            "   Train: 2144, Val: 459, Test: 461\n"
          ]
        }
      ],
      "source": [
        "# DATASET CLASS (FIXED FOR 3-WAY SPLIT)\n",
        "\n",
        "class BrainTumorDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Flexible dataset loader that auto-detects folder structure\n",
        "    Supports Training/Validation/Testing splits\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, split='Training', transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Auto-detect structure\n",
        "        root_path = Path(root_dir)\n",
        "\n",
        "        # Possible split names\n",
        "        split_aliases = {\n",
        "            'Training': ['Training', 'train', 'Train'],\n",
        "            'Validation': ['Validation', 'val', 'Val', 'valid', 'Valid'],\n",
        "            'Testing': ['Testing', 'test', 'Test']\n",
        "        }\n",
        "\n",
        "        # Find the correct split folder\n",
        "        base_path = None\n",
        "\n",
        "        # Check direct path\n",
        "        for alias in split_aliases.get(split, [split]):\n",
        "            if (root_path / alias).exists():\n",
        "                base_path = root_path / alias\n",
        "                break\n",
        "\n",
        "        # Check inside Figshare_Dataset folder\n",
        "        if base_path is None:\n",
        "            for alias in split_aliases.get(split, [split]):\n",
        "                if (root_path / 'Figshare_Dataset' / alias).exists():\n",
        "                    base_path = root_path / 'Figshare_Dataset' / alias\n",
        "                    break\n",
        "\n",
        "        # Search recursively\n",
        "        if base_path is None:\n",
        "            for alias in split_aliases.get(split, [split]):\n",
        "                possible_paths = list(root_path.rglob(alias))\n",
        "                if possible_paths:\n",
        "                    base_path = possible_paths[0]\n",
        "                    break\n",
        "\n",
        "        if base_path is None:\n",
        "            raise FileNotFoundError(\n",
        "                f\"Cannot find {split} folder in {root_dir}\\n\"\n",
        "                f\"Available folders: {[str(p) for p in root_path.rglob('*') if p.is_dir()]}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Loading {split} from: {base_path}\")\n",
        "\n",
        "        # Load images\n",
        "        for idx, class_name in enumerate(config.CLASSES):\n",
        "            class_dir = base_path / class_name\n",
        "            if not class_dir.exists():\n",
        "                print(f\"  Warning: {class_dir} not found, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Support both .jpg and .png\n",
        "            img_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))\n",
        "\n",
        "            for img_path in img_files:\n",
        "                self.images.append(str(img_path))\n",
        "                self.labels.append(idx)\n",
        "\n",
        "            print(f\"  {class_name}: {len(img_files)} images\")\n",
        "\n",
        "        print(f\"Total: {len(self.images)} images\\n\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Test dataset loading\n",
        "print(\" Testing dataset loading...\")\n",
        "try:\n",
        "    train_ds = BrainTumorDataset(config.NORMAL_SPLIT_ROOT, 'Training')\n",
        "    val_ds = BrainTumorDataset(config.NORMAL_SPLIT_ROOT, 'Validation')\n",
        "    test_ds = BrainTumorDataset(config.NORMAL_SPLIT_ROOT, 'Testing')\n",
        "    print(f\" Normal split loaded successfully!\")\n",
        "    print(f\"   Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
        "except Exception as e:\n",
        "    print(f\" Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t9Qw2cdQQCT",
        "outputId": "686bb847-5449-4abe-aaa8-278390150e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DataLoaders created successfully\n"
          ]
        }
      ],
      "source": [
        "# CREATE DATALOADERS\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(\" DataLoaders created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NurWc0CfKlTr",
        "outputId": "39eec058-3d09-4d0f-ef15-c4791dcf102b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Transforms defined!\n"
          ]
        }
      ],
      "source": [
        "# DATA AUGMENTATION\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\" Transforms defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j6JxlnNKuc4",
        "outputId": "a80216f5-1d8b-4769-8832-af01cf767617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Attention modules defined!\n"
          ]
        }
      ],
      "source": [
        "# ATTENTION MECHANISMS\n",
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"Channel Attention Module (from CBAM)\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "\n",
        "        # Channel attention\n",
        "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
        "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
        "\n",
        "        channel_att = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
        "        return x * channel_att\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"Spatial Attention Module (from CBAM)\"\"\"\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Spatial attention\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        spatial_att = torch.cat([avg_out, max_out], dim=1)\n",
        "        spatial_att = self.sigmoid(self.conv(spatial_att))\n",
        "\n",
        "        return x * spatial_att\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.channel_att = ChannelAttention(channels, reduction)\n",
        "        self.spatial_att = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_att(x)\n",
        "        x = self.spatial_att(x)\n",
        "        return x\n",
        "\n",
        "print(\" Attention modules defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-_iyPJeSv7_",
        "outputId": "656e69df-d45c-4f41-d041-220e52bcf262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All 4 models defined successfully!\n",
            " Models:\n",
            "  1. ResNet-50\n",
            "  2. DenseNet-121\n",
            "  3. EfficientNet-B0\n",
            "  5. Attention-Enhanced Swin (CBAM)\n"
          ]
        }
      ],
      "source": [
        "# MODEL ARCHITECTURES\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import models\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# ResNet-50 Baseline\n",
        "# -------------------------------------------------------------------------\n",
        "class ResNet50Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet50(pretrained=pretrained)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# DenseNet-121\n",
        "# -------------------------------------------------------------------------\n",
        "class DenseNet121Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.model = models.densenet121(pretrained=pretrained)\n",
        "        self.model.classifier = nn.Linear(\n",
        "            self.model.classifier.in_features, num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# EfficientNet-B0\n",
        "# -------------------------------------------------------------------------\n",
        "class EfficientNetB0Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=3, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            'efficientnet_b0',\n",
        "            pretrained=pretrained,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Attention-Enhanced Swin (CBAM + SAFE FEATURE HANDLING)\n",
        "# -------------------------------------------------------------------------\n",
        "class AttentionEnhancedSwin(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer + CBAM + custom classifier\n",
        "    SAFE across timm versions\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=3, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Swin backbone WITHOUT classifier or pooling\n",
        "        self.swin_backbone = timm.create_model(\n",
        "            'swin_tiny_patch4_window7_224',\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,\n",
        "            global_pool=''\n",
        "        )\n",
        "\n",
        "        self.feature_dim = self.swin_backbone.num_features  # 768\n",
        "\n",
        "        # CBAM attention\n",
        "        self.cbam = CBAM(self.feature_dim, reduction=16)\n",
        "\n",
        "        # Global pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -------- SAFE feature extraction --------\n",
        "        features = self.swin_backbone.forward_features(x)\n",
        "\n",
        "        # Case 1: [B, N, C] → reshape to [B, C, H, W]\n",
        "        if features.dim() == 3:\n",
        "            B, N, C = features.shape\n",
        "            H = W = int(N ** 0.5)\n",
        "            features = features.transpose(1, 2).contiguous().view(B, C, H, W)\n",
        "\n",
        "        # Case 2: [B, H, W, C] → NCHW\n",
        "        elif features.dim() == 4 and features.shape[-1] == self.feature_dim:\n",
        "            features = features.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # -------- Attention + pooling --------\n",
        "        features = self.cbam(features)\n",
        "        features = self.global_pool(features)\n",
        "        features = features.flatten(1)\n",
        "\n",
        "        # -------- Classification --------\n",
        "        return self.classifier(features)\n",
        "\n",
        "\n",
        "print(\" All 4 models defined successfully!\")\n",
        "print(\" Models:\")\n",
        "print(\"  1. ResNet-50\")\n",
        "print(\"  2. DenseNet-121\")\n",
        "print(\"  3. EfficientNet-B0\")\n",
        "print(\"  5. Attention-Enhanced Swin (CBAM)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq7nJssrVoQO"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm as tqdm_bar\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# TRAIN ONE EPOCH\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm_bar(\n",
        "        loader,\n",
        "        desc=f\"Epoch {epoch+1} [Train]\",\n",
        "        ascii=True,\n",
        "        leave=True,\n",
        "        mininterval=1.0\n",
        "    )\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix_str(\n",
        "            f\"loss={running_loss/total:.4f}, acc={100.*correct/total:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return running_loss / total, 100. * correct / total\n",
        "\n",
        "# EVALUATION (WITH TQDM)\n",
        "def evaluate(model, loader, criterion, device, desc=\"Eval\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    pbar = tqdm_bar(\n",
        "        loader,\n",
        "        desc=desc,\n",
        "        ascii=True,\n",
        "        leave=False,\n",
        "        mininterval=1.0\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            pbar.set_postfix_str(\n",
        "                f\"loss={running_loss/len(all_labels):.4f}\"\n",
        "            )\n",
        "\n",
        "    acc = 100. * accuracy_score(all_labels, all_preds)\n",
        "    return running_loss / len(all_labels), acc, np.array(all_preds), np.array(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOwatJuoLQDr",
        "outputId": "de787bfe-42bf-44ce-d250-850a36a4ab53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training functions ready!\n"
          ]
        }
      ],
      "source": [
        "def plot_confusion_matrix(cm, class_names, model_name, save_path):\n",
        "    \"\"\"Plot and save confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'{model_name}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_training_history(history, model_name, save_path):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    ax1.plot(history['test_loss'], label='Test Loss', marker='s')\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title(f'{model_name} - Loss Curves', fontsize=13, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "    ax2.plot(history['test_acc'], label='Test Accuracy', marker='s')\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title(f'{model_name} - Accuracy Curves', fontsize=13, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "print(\" Training functions ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_rIGpxqNanS"
      },
      "outputs": [],
      "source": [
        "def train_model(model_name, model, train_loader, val_loader, test_loader,\n",
        "                epochs, device, save_dir):\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TRAINING: {model_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.LR,\n",
        "        weight_decay=config.WEIGHT_DECAY\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"train_acc\": [],\n",
        "        \"val_loss\": [], \"val_acc\": [],\n",
        "        \"test_loss\": [], \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience = 5\n",
        "    wait = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, epoch\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, _, _ = evaluate(\n",
        "            model, val_loader, criterion, device, f\"Epoch {epoch+1} [Val]\"\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, _, _ = evaluate(\n",
        "            model, test_loader, criterion, device, f\"Epoch {epoch+1} [Test]\"\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch+1}/{epochs} | \"\n",
        "            f\"Train: {train_acc:.2f}% | \"\n",
        "            f\"Val: {val_acc:.2f}% | \"\n",
        "            f\"Test: {test_acc:.2f}%\"\n",
        "        )\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            wait = 0\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                save_dir / f\"{model_name}_best.pth\"\n",
        "            )\n",
        "            print( \"Best model saved\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\" Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(save_dir / f\"{model_name}_best.pth\"))\n",
        "\n",
        "    _, final_acc, preds, labels = evaluate(\n",
        "        model, test_loader, criterion, device, \"Final Test\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Test Accuracy:\", final_acc)\n",
        "\n",
        "    return {\n",
        "        \"history\": history,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"final_test_acc\": final_acc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN ALL MODELS\n",
        "\n",
        "def run_experiment(split_name, dataset_root):\n",
        "    \"\"\"Run complete experiment with Train/Val/Test split\"\"\"\n",
        "\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    print(f\"###  EXPERIMENT: {split_name.upper()}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    print(f\"{'#'*80}\\n\")\n",
        "\n",
        "    # Create save directory\n",
        "    save_dir = Path(config.RESULTS_DIR) / split_name\n",
        "    save_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # Load datasets (3-way split)\n",
        "    print(\" Loading datasets...\")\n",
        "    train_dataset = BrainTumorDataset(dataset_root, 'Training', train_transform)\n",
        "    val_dataset = BrainTumorDataset(dataset_root, 'Validation', test_transform)  # No augmentation for val\n",
        "    test_dataset = BrainTumorDataset(dataset_root, 'Testing', test_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\" Train: {len(train_dataset)} samples\")\n",
        "    print(f\" Val:   {len(val_dataset)} samples\")\n",
        "    print(f\" Test:  {len(test_dataset)} samples\\n\")\n",
        "\n",
        "    # Define models\n",
        "    models_dict = {\n",
        "        'ResNet-50': ResNet50Classifier(config.NUM_CLASSES),\n",
        "        'DenseNet-121': DenseNet121Classifier(config.NUM_CLASSES),\n",
        "        'EfficientNet-B0': EfficientNetB0Classifier(config.NUM_CLASSES),\n",
        "        'Attention-Enhanced-Swin': AttentionEnhancedSwin(config.NUM_CLASSES)\n",
        "    }\n",
        "\n",
        "    # Train all models\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name, model in models_dict.items():\n",
        "        results = train_model(\n",
        "            model_name, model,\n",
        "            train_loader, val_loader, test_loader,  # ← Now 3 loaders\n",
        "            config.EPOCHS, config.DEVICE,\n",
        "            save_dir\n",
        "        )\n",
        "        all_results[model_name] = results\n",
        "\n",
        "        # Save intermediate results\n",
        "        torch.save(all_results, save_dir / 'all_results.pth')\n",
        "\n",
        "    # Create results summary\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Model': list(all_results.keys()),\n",
        "        'Best Val Acc (%)': [all_results[m]['best_val_acc'] for m in all_results.keys()],\n",
        "        'Final Test Acc (%)': [all_results[m]['final_test_acc'] for m in all_results.keys()]\n",
        "    })\n",
        "\n",
        "    summary_df = summary_df.sort_values('Final Test Acc (%)', ascending=False)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\" FINAL RESULTS - {split_name.upper()}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    print(summary_df.to_string(index=False))\n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "    # Save results\n",
        "    summary_df.to_csv(save_dir / 'results_summary.csv', index=False)\n",
        "\n",
        "    # Val accuracy comparison\n",
        "    ax1.bar(summary_df['Model'], summary_df['Best Val Acc (%)'],\n",
        "            color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "    ax1.set_xlabel('Model', fontweight='bold', fontsize=11)\n",
        "    ax1.set_ylabel('Validation Accuracy (%)', fontweight='bold', fontsize=11)\n",
        "    ax1.set_title(f'Best Validation Accuracy - {split_name.upper()}',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    ax1.set_ylim([85, 100])\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, (model, acc) in enumerate(zip(summary_df['Model'], summary_df['Best Val Acc (%)'])):\n",
        "        ax1.text(i, acc + 0.5, f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "\n",
        "    # Test accuracy comparison\n",
        "    bars = ax2.bar(summary_df['Model'], summary_df['Final Test Acc (%)'],\n",
        "                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "    ax2.set_xlabel('Model', fontweight='bold', fontsize=11)\n",
        "    ax2.set_ylabel('Test Accuracy (%)', fontweight='bold', fontsize=11)\n",
        "    ax2.set_title(f'Final Test Accuracy - {split_name.upper()}',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.set_ylim([85, 100])\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for i, (model, acc) in enumerate(zip(summary_df['Model'], summary_df['Final Test Acc (%)'])):\n",
        "        ax2.text(i, acc + 0.5, f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return all_results, summary_df\n",
        "\n",
        "# RUN NORMAL SPLIT EXPERIMENT\n",
        "\n",
        "print(\" Starting NORMAL SPLIT experiment...\")\n",
        "normal_results, normal_summary = run_experiment('normal_split', config.NORMAL_SPLIT_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4V-dSbdjAT",
        "outputId": "95fa1f48-b404-4123-9137-f7de57e4e480"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting NORMAL SPLIT experiment...\n",
            "\n",
            "################################################################################\n",
            "################################################################################\n",
            "###  EXPERIMENT: NORMAL_SPLIT\n",
            "################################################################################\n",
            "################################################################################\n",
            "\n",
            "Loading datasets...\n",
            "Loading Training from: /content/datasets/normal_split/Figshare_ImageLevel/Training\n",
            " glioma: 467 images\n",
            " meningioma: 1021 images\n",
            " pituitary: 656 images\n",
            "Total: 2144 images\n",
            "\n",
            "Loading Validation from: /content/datasets/normal_split/Figshare_ImageLevel/Validation\n",
            " glioma: 131 images\n",
            " meningioma: 207 images\n",
            " pituitary: 121 images\n",
            "Total: 459 images\n",
            "\n",
            "Loading Testing from: /content/datasets/normal_split/Figshare_ImageLevel/Testing\n",
            " glioma: 110 images\n",
            " meningioma: 198 images\n",
            " pituitary: 153 images\n",
            "Total: 461 images\n",
            "\n",
            "Train: 2144 samples\n",
            "Val:   459 samples\n",
            "Test:  461 samples\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TRAINING: ResNet-50\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.3265, acc=86.47%]\n",
            "\n",
            "Epoch 1/10 | Train: 86.47% | Val: 94.77% | Test: 93.71%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 67/67 [00:38<00:00,  1.73it/s, loss=0.1582, acc=94.50%]\n",
            "\n",
            "Epoch 2/10 | Train: 94.50% | Val: 97.39% | Test: 95.23%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.69it/s, loss=0.1129, acc=96.18%]\n",
            "\n",
            "Epoch 3/10 | Train: 96.18% | Val: 97.17% | Test: 96.53%\n",
            "Epoch 4 [Train]: 100%|##########| 67/67 [00:38<00:00,  1.73it/s, loss=0.0914, acc=96.50%]\n",
            "\n",
            "Epoch 4/10 | Train: 96.50% | Val: 97.60% | Test: 97.40%\n",
            "Best model saved\n",
            "Epoch 5 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.0807, acc=96.88%]\n",
            "\n",
            "Epoch 5/10 | Train: 96.88% | Val: 96.95% | Test: 95.88%\n",
            "Epoch 6 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.0643, acc=97.76%]\n",
            "\n",
            "Epoch 6/10 | Train: 97.76% | Val: 98.04% | Test: 97.40%\n",
            "Best model saved\n",
            "Epoch 7 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.0428, acc=98.46%]\n",
            "\n",
            "Epoch 7/10 | Train: 98.46% | Val: 97.82% | Test: 98.48%\n",
            "Epoch 8 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.0287, acc=99.07%]\n",
            "\n",
            "Epoch 8/10 | Train: 99.07% | Val: 97.82% | Test: 98.26%\n",
            "Epoch 9 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.0179, acc=99.63%]\n",
            "\n",
            "Epoch 9/10 | Train: 99.63% | Val: 98.04% | Test: 98.48%\n",
            "Epoch 10 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.70it/s, loss=0.0214, acc=99.30%]\n",
            "\n",
            "Epoch 10/10 | Train: 99.30% | Val: 98.47% | Test: 98.26%\n",
            "Best model saved\n",
            "\n",
            "Final Test Accuracy: 98.2646420824295\n",
            "\n",
            "======================================================================\n",
            "TRAINING: DenseNet-121\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 67/67 [00:40<00:00,  1.66it/s, loss=0.4236, acc=83.30%]\n",
            "\n",
            "Epoch 1/10 | Train: 83.30% | Val: 93.25% | Test: 93.49%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 67/67 [00:40<00:00,  1.66it/s, loss=0.1713, acc=93.42%]\n",
            "\n",
            "Epoch 2/10 | Train: 93.42% | Val: 94.55% | Test: 94.79%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 67/67 [00:39<00:00,  1.68it/s, loss=0.1151, acc=95.94%]\n",
            "\n",
            "Epoch 3/10 | Train: 95.94% | Val: 93.46% | Test: 94.36%\n",
            "Epoch 4 [Train]: 100%|##########| 67/67 [00:48<00:00,  1.39it/s, loss=0.0951, acc=96.69%]\n",
            "\n",
            "Epoch 4/10 | Train: 96.69% | Val: 97.82% | Test: 97.18%\n",
            "Best model saved\n",
            "Epoch 5 [Train]: 100%|##########| 67/67 [00:40<00:00,  1.67it/s, loss=0.0670, acc=97.76%]\n",
            "\n",
            "Epoch 5/10 | Train: 97.76% | Val: 97.60% | Test: 96.10%\n",
            "Epoch 6 [Train]: 100%|##########| 67/67 [00:42<00:00,  1.59it/s, loss=0.0518, acc=98.09%]\n",
            "\n",
            "Epoch 6/10 | Train: 98.09% | Val: 98.47% | Test: 97.83%\n",
            "Best model saved\n",
            "Epoch 7 [Train]: 100%|##########| 67/67 [00:42<00:00,  1.60it/s, loss=0.0353, acc=98.83%]\n",
            "\n",
            "Epoch 7/10 | Train: 98.83% | Val: 98.26% | Test: 98.05%\n",
            "Epoch 8 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.0286, acc=99.53%]\n",
            "\n",
            "Epoch 8/10 | Train: 99.53% | Val: 98.69% | Test: 98.48%\n",
            "Best model saved\n",
            "Epoch 9 [Train]: 100%|##########| 67/67 [00:40<00:00,  1.65it/s, loss=0.0215, acc=99.53%]\n",
            "\n",
            "Epoch 9/10 | Train: 99.53% | Val: 98.91% | Test: 98.48%\n",
            "Best model saved\n",
            "Epoch 10 [Train]: 100%|##########| 67/67 [00:41<00:00,  1.63it/s, loss=0.0210, acc=99.44%]\n",
            "\n",
            "Epoch 10/10 | Train: 99.44% | Val: 98.26% | Test: 97.61%\n",
            "\n",
            "Final Test Accuracy: 98.48156182212581\n",
            "\n",
            "======================================================================\n",
            "TRAINING: EfficientNet-B0\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 67/67 [00:31<00:00,  2.14it/s, loss=0.9701, acc=76.35%]\n",
            "\n",
            "Epoch 1/10 | Train: 76.35% | Val: 85.40% | Test: 86.77%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 67/67 [00:30<00:00,  2.23it/s, loss=0.4107, acc=88.15%]\n",
            "\n",
            "Epoch 2/10 | Train: 88.15% | Val: 89.54% | Test: 88.94%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.25it/s, loss=0.2848, acc=90.90%]\n",
            "\n",
            "Epoch 3/10 | Train: 90.90% | Val: 92.16% | Test: 91.11%\n",
            "Best model saved\n",
            "Epoch 4 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.26it/s, loss=0.2487, acc=91.74%]\n",
            "\n",
            "Epoch 4/10 | Train: 91.74% | Val: 92.59% | Test: 93.28%\n",
            "Best model saved\n",
            "Epoch 5 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.27it/s, loss=0.1536, acc=94.54%]\n",
            "\n",
            "Epoch 5/10 | Train: 94.54% | Val: 93.25% | Test: 92.62%\n",
            "Best model saved\n",
            "Epoch 6 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.27it/s, loss=0.1306, acc=95.20%]\n",
            "\n",
            "Epoch 6/10 | Train: 95.20% | Val: 95.21% | Test: 95.44%\n",
            "Best model saved\n",
            "Epoch 7 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.29it/s, loss=0.1381, acc=94.50%]\n",
            "\n",
            "Epoch 7/10 | Train: 94.50% | Val: 93.68% | Test: 93.93%\n",
            "Epoch 8 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.28it/s, loss=0.1132, acc=95.90%]\n",
            "\n",
            "Epoch 8/10 | Train: 95.90% | Val: 95.42% | Test: 95.66%\n",
            "Best model saved\n",
            "Epoch 9 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.29it/s, loss=0.1040, acc=95.90%]\n",
            "\n",
            "Epoch 9/10 | Train: 95.90% | Val: 94.77% | Test: 94.79%\n",
            "Epoch 10 [Train]: 100%|##########| 67/67 [00:29<00:00,  2.29it/s, loss=0.0932, acc=97.06%]\n",
            "\n",
            "Epoch 10/10 | Train: 97.06% | Val: 95.42% | Test: 95.01%\n",
            "\n",
            "Final Test Accuracy: 95.66160520607376\n",
            "\n",
            "======================================================================\n",
            "TRAINING: Attention-Enhanced-Swin\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.6501, acc=73.04%]\n",
            "\n",
            "Epoch 1/10 | Train: 73.04% | Val: 84.53% | Test: 83.08%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.2795, acc=91.18%]\n",
            "\n",
            "Epoch 2/10 | Train: 91.18% | Val: 90.85% | Test: 91.76%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.52it/s, loss=0.2019, acc=93.94%]\n",
            "\n",
            "Epoch 3/10 | Train: 93.94% | Val: 94.77% | Test: 95.23%\n",
            "Best model saved\n",
            "Epoch 4 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.1441, acc=95.29%]\n",
            "\n",
            "Epoch 4/10 | Train: 95.29% | Val: 93.68% | Test: 93.93%\n",
            "Epoch 5 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.52it/s, loss=0.1047, acc=97.11%]\n",
            "\n",
            "Epoch 5/10 | Train: 97.11% | Val: 97.39% | Test: 96.96%\n",
            "Best model saved\n",
            "Epoch 6 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.0974, acc=97.01%]\n",
            "\n",
            "Epoch 6/10 | Train: 97.01% | Val: 96.73% | Test: 96.75%\n",
            "Epoch 7 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.0970, acc=97.01%]\n",
            "\n",
            "Epoch 7/10 | Train: 97.01% | Val: 98.47% | Test: 98.05%\n",
            "Best model saved\n",
            "Epoch 8 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.51it/s, loss=0.0629, acc=98.65%]\n",
            "\n",
            "Epoch 8/10 | Train: 98.65% | Val: 98.47% | Test: 98.26%\n",
            "Epoch 9 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.50it/s, loss=0.0640, acc=98.46%]\n",
            "\n",
            "Epoch 9/10 | Train: 98.46% | Val: 98.26% | Test: 98.26%\n",
            "Epoch 10 [Train]: 100%|##########| 67/67 [00:44<00:00,  1.52it/s, loss=0.0631, acc=98.23%]\n",
            "\n",
            "Epoch 10/10 | Train: 98.23% | Val: 98.47% | Test: 98.05%\n",
            "\n",
            "Final Test Accuracy: 98.0477223427332\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS - NORMAL_SPLIT\n",
            "================================================================================\n",
            "\n",
            "                  Model  Best Val Acc (%)  Final Test Acc (%)\n",
            "           DenseNet-121         98.910675           98.481562\n",
            "              ResNet-50         98.474946           98.264642\n",
            "Attention-Enhanced-Swin         98.474946           98.047722\n",
            "        EfficientNet-B0         95.424837           95.661605\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN PATIENT-LEVEL SPLIT EXPERIMENT\n",
        "\n",
        "print(\"\\n\\n Starting PATIENT-LEVEL SPLIT experiment...\")\n",
        "patient_results, patient_summary = run_experiment('patient_split', config.PATIENT_SPLIT_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50msHj_yepIu",
        "outputId": "e7f1fc8c-6076-433d-8a60-b6969c095314"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting PATIENT-LEVEL SPLIT experiment...\n",
            "\n",
            "################################################################################\n",
            "################################################################################\n",
            "###  EXPERIMENT: PATIENT_SPLIT\n",
            "################################################################################\n",
            "################################################################################\n",
            "\n",
            " Loading datasets...\n",
            "Loading Training from: /content/datasets/patient_split/Figshare_Dataset/Training\n",
            "  glioma: 1002 images\n",
            "  meningioma: 509 images\n",
            "  pituitary: 648 images\n",
            "Total: 2159 images\n",
            "\n",
            "Loading Validation from: /content/datasets/patient_split/Figshare_Dataset/Validation\n",
            "  glioma: 223 images\n",
            "  meningioma: 80 images\n",
            "  pituitary: 130 images\n",
            "Total: 433 images\n",
            "\n",
            "Loading Testing from: /content/datasets/patient_split/Figshare_Dataset/Testing\n",
            "  glioma: 201 images\n",
            "  meningioma: 119 images\n",
            "  pituitary: 152 images\n",
            "Total: 472 images\n",
            "\n",
            " Train: 2159 samples\n",
            " Val:   433 samples\n",
            " Test:  472 samples\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TRAINING: ResNet-50\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 68/68 [00:39<00:00,  1.71it/s, loss=0.3618, acc=85.83%]\n",
            "\n",
            "Epoch 1/10 | Train: 85.83% | Val: 90.07% | Test: 92.16%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.86it/s, loss=0.1562, acc=94.21%]\n",
            "\n",
            "Epoch 2/10 | Train: 94.21% | Val: 90.99% | Test: 96.19%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.86it/s, loss=0.1206, acc=95.83%]\n",
            "\n",
            "Epoch 3/10 | Train: 95.83% | Val: 92.38% | Test: 95.34%\n",
            "Best model saved\n",
            "Epoch 4 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.88it/s, loss=0.0867, acc=97.17%]\n",
            "\n",
            "Epoch 4/10 | Train: 97.17% | Val: 94.92% | Test: 96.61%\n",
            "Best model saved\n",
            "Epoch 5 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.85it/s, loss=0.0813, acc=96.99%]\n",
            "\n",
            "Epoch 5/10 | Train: 96.99% | Val: 91.22% | Test: 92.58%\n",
            "Epoch 6 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.86it/s, loss=0.0433, acc=98.75%]\n",
            "\n",
            "Epoch 6/10 | Train: 98.75% | Val: 93.53% | Test: 95.55%\n",
            "Epoch 7 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.85it/s, loss=0.0288, acc=99.31%]\n",
            "\n",
            "Epoch 7/10 | Train: 99.31% | Val: 95.15% | Test: 95.97%\n",
            "Best model saved\n",
            "Epoch 8 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.85it/s, loss=0.0208, acc=99.40%]\n",
            "\n",
            "Epoch 8/10 | Train: 99.40% | Val: 94.23% | Test: 95.55%\n",
            "Epoch 9 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.86it/s, loss=0.0220, acc=99.40%]\n",
            "\n",
            "Epoch 9/10 | Train: 99.40% | Val: 94.23% | Test: 96.19%\n",
            "Epoch 10 [Train]: 100%|##########| 68/68 [00:36<00:00,  1.88it/s, loss=0.0224, acc=99.31%]\n",
            "\n",
            "Epoch 10/10 | Train: 99.31% | Val: 94.69% | Test: 96.19%\n",
            "\n",
            "Final Test Accuracy: 95.97457627118644\n",
            "\n",
            "======================================================================\n",
            "TRAINING: DenseNet-121\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.83it/s, loss=0.4097, acc=83.88%]\n",
            "\n",
            "Epoch 1/10 | Train: 83.88% | Val: 92.84% | Test: 93.64%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.82it/s, loss=0.1649, acc=94.49%]\n",
            "\n",
            "Epoch 2/10 | Train: 94.49% | Val: 94.92% | Test: 95.97%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.83it/s, loss=0.1161, acc=95.74%]\n",
            "\n",
            "Epoch 3/10 | Train: 95.74% | Val: 93.30% | Test: 93.86%\n",
            "Epoch 4 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.83it/s, loss=0.0772, acc=97.50%]\n",
            "\n",
            "Epoch 4/10 | Train: 97.50% | Val: 90.99% | Test: 94.92%\n",
            "Epoch 5 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.83it/s, loss=0.0585, acc=98.10%]\n",
            "\n",
            "Epoch 5/10 | Train: 98.10% | Val: 94.00% | Test: 95.55%\n",
            "Epoch 6 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.83it/s, loss=0.0427, acc=98.66%]\n",
            "\n",
            "Epoch 6/10 | Train: 98.66% | Val: 93.07% | Test: 94.70%\n",
            "Epoch 7 [Train]: 100%|##########| 68/68 [00:37<00:00,  1.83it/s, loss=0.0411, acc=98.56%]\n",
            "\n",
            "Epoch 7/10 | Train: 98.56% | Val: 93.30% | Test: 95.34%\n",
            " Early stopping triggered\n",
            "\n",
            "Final Test Accuracy: 95.97457627118644\n",
            "\n",
            "======================================================================\n",
            "TRAINING: EfficientNet-B0\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 68/68 [00:25<00:00,  2.62it/s, loss=1.0853, acc=74.53%]\n",
            "\n",
            "Epoch 1/10 | Train: 74.53% | Val: 83.60% | Test: 84.11%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.59it/s, loss=0.4618, acc=86.71%]\n",
            "\n",
            "Epoch 2/10 | Train: 86.71% | Val: 85.45% | Test: 90.47%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.58it/s, loss=0.3164, acc=89.58%]\n",
            "\n",
            "Epoch 3/10 | Train: 89.58% | Val: 87.07% | Test: 88.35%\n",
            "Best model saved\n",
            "Epoch 4 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.60it/s, loss=0.2514, acc=92.13%]\n",
            "\n",
            "Epoch 4/10 | Train: 92.13% | Val: 87.30% | Test: 91.10%\n",
            "Best model saved\n",
            "Epoch 5 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.57it/s, loss=0.1686, acc=93.98%]\n",
            "\n",
            "Epoch 5/10 | Train: 93.98% | Val: 86.37% | Test: 90.68%\n",
            "Epoch 6 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.60it/s, loss=0.1588, acc=94.86%]\n",
            "\n",
            "Epoch 6/10 | Train: 94.86% | Val: 86.61% | Test: 91.10%\n",
            "Epoch 7 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.56it/s, loss=0.1256, acc=95.51%]\n",
            "\n",
            "Epoch 7/10 | Train: 95.51% | Val: 86.61% | Test: 91.74%\n",
            "Epoch 8 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.59it/s, loss=0.1196, acc=95.88%]\n",
            "\n",
            "Epoch 8/10 | Train: 95.88% | Val: 87.99% | Test: 91.95%\n",
            "Best model saved\n",
            "Epoch 9 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.57it/s, loss=0.0979, acc=96.48%]\n",
            "\n",
            "Epoch 9/10 | Train: 96.48% | Val: 87.07% | Test: 92.16%\n",
            "Epoch 10 [Train]: 100%|##########| 68/68 [00:26<00:00,  2.55it/s, loss=0.1023, acc=96.34%]\n",
            "\n",
            "Epoch 10/10 | Train: 96.34% | Val: 87.99% | Test: 92.58%\n",
            "\n",
            "Final Test Accuracy: 91.94915254237289\n",
            "\n",
            "======================================================================\n",
            "TRAINING: Attention-Enhanced-Swin\n",
            "======================================================================\n",
            "Epoch 1 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.64it/s, loss=0.5626, acc=76.84%]\n",
            "\n",
            "Epoch 1/10 | Train: 76.84% | Val: 87.30% | Test: 91.74%\n",
            "Best model saved\n",
            "Epoch 2 [Train]: 100%|##########| 68/68 [00:42<00:00,  1.62it/s, loss=0.2112, acc=92.91%]\n",
            "\n",
            "Epoch 2/10 | Train: 92.91% | Val: 87.99% | Test: 90.25%\n",
            "Best model saved\n",
            "Epoch 3 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.62it/s, loss=0.2071, acc=92.87%]\n",
            "\n",
            "Epoch 3/10 | Train: 92.87% | Val: 93.76% | Test: 98.52%\n",
            "Best model saved\n",
            "Epoch 4 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.63it/s, loss=0.1306, acc=95.79%]\n",
            "\n",
            "Epoch 4/10 | Train: 95.79% | Val: 95.38% | Test: 97.46%\n",
            "Best model saved\n",
            "Epoch 5 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.62it/s, loss=0.1034, acc=97.13%]\n",
            "\n",
            "Epoch 5/10 | Train: 97.13% | Val: 95.84% | Test: 96.19%\n",
            "Best model saved\n",
            "Epoch 6 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.64it/s, loss=0.1017, acc=96.62%]\n",
            "\n",
            "Epoch 6/10 | Train: 96.62% | Val: 94.00% | Test: 97.03%\n",
            "Epoch 7 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.64it/s, loss=0.0748, acc=97.96%]\n",
            "\n",
            "Epoch 7/10 | Train: 97.96% | Val: 91.92% | Test: 96.40%\n",
            "Epoch 8 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.64it/s, loss=0.0552, acc=98.52%]\n",
            "\n",
            "Epoch 8/10 | Train: 98.52% | Val: 94.00% | Test: 97.03%\n",
            "Epoch 9 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.63it/s, loss=0.0581, acc=98.70%]\n",
            "\n",
            "Epoch 9/10 | Train: 98.70% | Val: 96.07% | Test: 96.82%\n",
            "Best model saved\n",
            "Epoch 10 [Train]: 100%|##########| 68/68 [00:41<00:00,  1.64it/s, loss=0.0461, acc=98.93%]\n",
            "\n",
            "Epoch 10/10 | Train: 98.93% | Val: 94.92% | Test: 96.82%\n",
            "\n",
            "Final Test Accuracy: 96.82203389830508\n",
            "\n",
            "================================================================================\n",
            " FINAL RESULTS - PATIENT_SPLIT\n",
            "================================================================================\n",
            "\n",
            "                  Model  Best Val Acc (%)  Final Test Acc (%)\n",
            "Attention-Enhanced-Swin         96.073903           96.822034\n",
            "              ResNet-50         95.150115           95.974576\n",
            "           DenseNet-121         94.919169           95.974576\n",
            "        EfficientNet-B0         87.990762           91.949153\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Po4gTpp_Qe",
        "outputId": "f60604c0-a38a-4cb1-cf6a-c1b287458b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Locating model file...\n",
            " Found weights: results/patient_split/Attention-Enhanced-Swin_best.pth\n",
            " Error during visualization (skipping): Error(s) in loading state_dict for AttentionEnhancedSwin:\n",
            "\tMissing key(s) in state_dict: \"swin.patch_embed.proj.weight\", \"swin.patch_embed.proj.bias\", \"swin.patch_embed.norm.weight\", \"swin.patch_embed.norm.bias\", \"swin.layers.0.blocks.0.norm1.weight\", \"swin.layers.0.blocks.0.norm1.bias\", \"swin.layers.0.blocks.0.attn.relative_position_bias_table\", \"swin.layers.0.blocks.0.attn.qkv.weight\", \"swin.layers.0.blocks.0.attn.qkv.bias\", \"swin.layers.0.blocks.0.attn.proj.weight\", \"swin.layers.0.blocks.0.attn.proj.bias\", \"swin.layers.0.blocks.0.norm2.weight\", \"swin.layers.0.blocks.0.norm2.bias\", \"swin.layers.0.blocks.0.mlp.fc1.weight\", \"swin.layers.0.blocks.0.mlp.fc1.bias\", \"swin.layers.0.blocks.0.mlp.fc2.weight\", \"swin.layers.0.blocks.0.mlp.fc2.bias\", \"swin.layers.0.blocks.1.norm1.weight\", \"swin.layers.0.blocks.1.norm1.bias\", \"swin.layers.0.blocks.1.attn.relative_position_bias_table\", \"swin.layers.0.blocks.1.attn.qkv.weight\", \"swin.layers.0.blocks.1.attn.qkv.bias\", \"swin.layers.0.blocks.1.attn.proj.weight\", \"swin.layers.0.blocks.1.attn.proj.bias\", \"swin.layers.0.blocks.1.norm2.weight\", \"swin.layers.0.blocks.1.norm2.bias\", \"swin.layers.0.blocks.1.mlp.fc1.weight\", \"swin.layers.0.blocks.1.mlp.fc1.bias\", \"swin.layers.0.blocks.1.mlp.fc2.weight\", \"swin.layers.0.blocks.1.mlp.fc2.bias\", \"swin.layers.1.downsample.norm.weight\", \"swin.layers.1.downsample.norm.bias\", \"swin.layers.1.downsample.reduction.weight\", \"swin.layers.1.blocks.0.norm1.weight\", \"swin.layers.1.blocks.0.norm1.bias\", \"swin.layers.1.blocks.0.attn.relative_position_bias_table\", \"swin.layers.1.blocks.0.attn.qkv.weight\", \"swin.layers.1.blocks.0.attn.qkv.bias\", \"swin.layers.1.blocks.0.attn.proj.weight\", \"swin.layers.1.blocks.0.attn.proj.bias\", \"swin.layers.1.blocks.0.norm2.weight\", \"swin.layers.1.blocks.0.norm2.bias\", \"swin.layers.1.blocks.0.mlp.fc1.weight\", \"swin.layers.1.blocks.0.mlp.fc1.bias\", \"swin.layers.1.blocks.0.mlp.fc2.weight\", \"swin.layers.1.blocks.0.mlp.fc2.bias\", \"swin.layers.1.blocks.1.norm1.weight\", \"swin.layers.1.blocks.1.norm1.bias\", \"swin.layers.1.blocks.1.attn.relative_position_bias_table\", \"swin.layers.1.blocks.1.attn.qkv.weight\", \"swin.layers.1.blocks.1.attn.qkv.bias\", \"swin.layers.1.blocks.1.attn.proj.weight\", \"swin.layers.1.blocks.1.attn.proj.bias\", \"swin.layers.1.blocks.1.norm2.weight\", \"swin.layers.1.blocks.1.norm2.bias\", \"swin.layers.1.blocks.1.mlp.fc1.weight\", \"swin.layers.1.blocks.1.mlp.fc1.bias\", \"swin.layers.1.blocks.1.mlp.fc2.weight\", \"swin.layers.1.blocks.1.mlp.fc2.bias\", \"swin.layers.2.downsample.norm.weight\", \"swin.layers.2.downsample.norm.bias\", \"swin.layers.2.downsample.reduction.weight\", \"swin.layers.2.blocks.0.norm1.weight\", \"swin.layers.2.blocks.0.norm1.bias\", \"swin.layers.2.blocks.0.attn.relative_position_bias_table\", \"swin.layers.2.blocks.0.attn.qkv.weight\", \"swin.layers.2.blocks.0.attn.qkv.bias\", \"swin.layers.2.blocks.0.attn.proj.weight\", \"swin.layers.2.blocks.0.attn.proj.bias\", \"swin.layers.2.blocks.0.norm2.weight\", \"swin.layers.2.blocks.0.norm2.bias\", \"swin.layers.2.blocks.0.mlp.fc1.weight\", \"swin.layers.2.blocks.0.mlp.fc1.bias\", \"swin.layers.2.blocks.0.mlp.fc2.weight\", \"swin.layers.2.blocks.0.mlp.fc2.bias\", \"swin.layers.2.blocks.1.norm1.weight\", \"swin.layers.2.blocks.1.norm1.bias\", \"swin.layers.2.blocks.1.attn.relative_position_bias_table\", \"swin.layers.2.blocks.1.attn.qkv.weight\", \"swin.layers.2.blocks.1.attn.qkv.bias\", \"swin.layers.2.blocks.1.attn.proj.weight\", \"swin.layers.2.blocks.1.attn.proj.bias\", \"swin.layers.2.blocks.1.norm2.weight\", \"swin.layers.2.blocks.1.norm2.bias\", \"swin.layers.2.blocks.1.mlp.fc1.weight\", \"swin.layers.2.blocks.1.mlp.fc1.bias\", \"swin.layers.2.blocks.1.mlp.fc2.weight\", \"swin.layers.2.blocks.1.mlp.fc2.bias\", \"swin.layers.2.blocks.2.norm1.weight\", \"swin.layers.2.blocks.2.norm1.bias\", \"swin.layers.2.blocks.2.attn.relative_position_bias_table\", \"swin.layers.2.blocks.2.attn.qkv.weight\", \"swin.layers.2.blocks.2.attn.qkv.bias\", \"swin.layers.2.blocks.2.attn.proj.weight\", \"swin.layers.2.blocks.2.attn.proj.bias\", \"swin.layers.2.blocks.2.norm2.weight\", \"swin.layers.2.blocks.2.norm2.bias\", \"swin.layers.2.blocks.2.mlp.fc1.weight\", \"swin.layers.2.blocks.2.mlp.fc1.bias\", \"swin.layers.2.blocks.2.mlp.fc2.weight\", \"swin.layers.2.blocks.2.mlp.fc2.bias\", \"swin.layers.2.blocks.3.norm1.weight\", \"swin.layers.2.blocks.3.norm1.bias\", \"swin.layers.2.blocks.3.attn.relative_position_bias_table\", \"swin.layers.2.blocks.3.attn.qkv.weight\", \"swin.layers.2.blocks.3.attn.qkv.bias\", \"swin.layers.2.blocks.3.attn.proj.weight\", \"swin.layers.2.blocks.3.attn.proj.bias\", \"swin.layers.2.blocks.3.norm2.weight\", \"swin.layers.2.blocks.3.norm2.bias\", \"swin.layers.2.blocks.3.mlp.fc1.weight\", \"swin.layers.2.blocks.3.mlp.fc1.bias\", \"swin.layers.2.blocks.3.mlp.fc2.weight\", \"swin.layers.2.blocks.3.mlp.fc2.bias\", \"swin.layers.2.blocks.4.norm1.weight\", \"swin.layers.2.blocks.4.norm1.bias\", \"swin.layers.2.blocks.4.attn.relative_position_bias_table\", \"swin.layers.2.blocks.4.attn.qkv.weight\", \"swin.layers.2.blocks.4.attn.qkv.bias\", \"swin.layers.2.blocks.4.attn.proj.weight\", \"swin.layers.2.blocks.4.attn.proj.bias\", \"swin.layers.2.blocks.4.norm2.weight\", \"swin.layers.2.blocks.4.norm2.bias\", \"swin.layers.2.blocks.4.mlp.fc1.weight\", \"swin.layers.2.blocks.4.mlp.fc1.bias\", \"swin.layers.2.blocks.4.mlp.fc2.weight\", \"swin.layers.2.blocks.4.mlp.fc2.bias\", \"swin.layers.2.blocks.5.norm1.weight\", \"swin.layers.2.blocks.5.norm1.bias\", \"swin.layers.2.blocks.5.attn.relative_position_bias_table\", \"swin.layers.2.blocks.5.attn.qkv.weight\", \"swin.layers.2.blocks.5.attn.qkv.bias\", \"swin.layers.2.blocks.5.attn.proj.weight\", \"swin.layers.2.blocks.5.attn.proj.bias\", \"swin.layers.2.blocks.5.norm2.weight\", \"swin.layers.2.blocks.5.norm2.bias\", \"swin.layers.2.blocks.5.mlp.fc1.weight\", \"swin.layers.2.blocks.5.mlp.fc1.bias\", \"swin.layers.2.blocks.5.mlp.fc2.weight\", \"swin.layers.2.blocks.5.mlp.fc2.bias\", \"swin.layers.3.downsample.norm.weight\", \"swin.layers.3.downsample.norm.bias\", \"swin.layers.3.downsample.reduction.weight\", \"swin.layers.3.blocks.0.norm1.weight\", \"swin.layers.3.blocks.0.norm1.bias\", \"swin.layers.3.blocks.0.attn.relative_position_bias_table\", \"swin.layers.3.blocks.0.attn.qkv.weight\", \"swin.layers.3.blocks.0.attn.qkv.bias\", \"swin.layers.3.blocks.0.attn.proj.weight\", \"swin.layers.3.blocks.0.attn.proj.bias\", \"swin.layers.3.blocks.0.norm2.weight\", \"swin.layers.3.blocks.0.norm2.bias\", \"swin.layers.3.blocks.0.mlp.fc1.weight\", \"swin.layers.3.blocks.0.mlp.fc1.bias\", \"swin.layers.3.blocks.0.mlp.fc2.weight\", \"swin.layers.3.blocks.0.mlp.fc2.bias\", \"swin.layers.3.blocks.1.norm1.weight\", \"swin.layers.3.blocks.1.norm1.bias\", \"swin.layers.3.blocks.1.attn.relative_position_bias_table\", \"swin.layers.3.blocks.1.attn.qkv.weight\", \"swin.layers.3.blocks.1.attn.qkv.bias\", \"swin.layers.3.blocks.1.attn.proj.weight\", \"swin.layers.3.blocks.1.attn.proj.bias\", \"swin.layers.3.blocks.1.norm2.weight\", \"swin.layers.3.blocks.1.norm2.bias\", \"swin.layers.3.blocks.1.mlp.fc1.weight\", \"swin.layers.3.blocks.1.mlp.fc1.bias\", \"swin.layers.3.blocks.1.mlp.fc2.weight\", \"swin.layers.3.blocks.1.mlp.fc2.bias\", \"swin.norm.weight\", \"swin.norm.bias\", \"cbam.ca.fc.0.weight\", \"cbam.ca.fc.2.weight\", \"cbam.sa.conv.weight\", \"classifier.3.weight\", \"classifier.3.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"swin_backbone.patch_embed.proj.weight\", \"swin_backbone.patch_embed.proj.bias\", \"swin_backbone.patch_embed.norm.weight\", \"swin_backbone.patch_embed.norm.bias\", \"swin_backbone.layers.0.blocks.0.norm1.weight\", \"swin_backbone.layers.0.blocks.0.norm1.bias\", \"swin_backbone.layers.0.blocks.0.attn.relative_position_bias_table\", \"swin_backbone.layers.0.blocks.0.attn.qkv.weight\", \"swin_backbone.layers.0.blocks.0.attn.qkv.bias\", \"swin_backbone.layers.0.blocks.0.attn.proj.weight\", \"swin_backbone.layers.0.blocks.0.attn.proj.bias\", \"swin_backbone.layers.0.blocks.0.norm2.weight\", \"swin_backbone.layers.0.blocks.0.norm2.bias\", \"swin_backbone.layers.0.blocks.0.mlp.fc1.weight\", \"swin_backbone.layers.0.blocks.0.mlp.fc1.bias\", \"swin_backbone.layers.0.blocks.0.mlp.fc2.weight\", \"swin_backbone.layers.0.blocks.0.mlp.fc2.bias\", \"swin_backbone.layers.0.blocks.1.norm1.weight\", \"swin_backbone.layers.0.blocks.1.norm1.bias\", \"swin_backbone.layers.0.blocks.1.attn.relative_position_bias_table\", \"swin_backbone.layers.0.blocks.1.attn.qkv.weight\", \"swin_backbone.layers.0.blocks.1.attn.qkv.bias\", \"swin_backbone.layers.0.blocks.1.attn.proj.weight\", \"swin_backbone.layers.0.blocks.1.attn.proj.bias\", \"swin_backbone.layers.0.blocks.1.norm2.weight\", \"swin_backbone.layers.0.blocks.1.norm2.bias\", \"swin_backbone.layers.0.blocks.1.mlp.fc1.weight\", \"swin_backbone.layers.0.blocks.1.mlp.fc1.bias\", \"swin_backbone.layers.0.blocks.1.mlp.fc2.weight\", \"swin_backbone.layers.0.blocks.1.mlp.fc2.bias\", \"swin_backbone.layers.1.downsample.norm.weight\", \"swin_backbone.layers.1.downsample.norm.bias\", \"swin_backbone.layers.1.downsample.reduction.weight\", \"swin_backbone.layers.1.blocks.0.norm1.weight\", \"swin_backbone.layers.1.blocks.0.norm1.bias\", \"swin_backbone.layers.1.blocks.0.attn.relative_position_bias_table\", \"swin_backbone.layers.1.blocks.0.attn.qkv.weight\", \"swin_backbone.layers.1.blocks.0.attn.qkv.bias\", \"swin_backbone.layers.1.blocks.0.attn.proj.weight\", \"swin_backbone.layers.1.blocks.0.attn.proj.bias\", \"swin_backbone.layers.1.blocks.0.norm2.weight\", \"swin_backbone.layers.1.blocks.0.norm2.bias\", \"swin_backbone.layers.1.blocks.0.mlp.fc1.weight\", \"swin_backbone.layers.1.blocks.0.mlp.fc1.bias\", \"swin_backbone.layers.1.blocks.0.mlp.fc2.weight\", \"swin_backbone.layers.1.blocks.0.mlp.fc2.bias\", \"swin_backbone.layers.1.blocks.1.norm1.weight\", \"swin_backbone.layers.1.blocks.1.norm1.bias\", \"swin_backbone.layers.1.blocks.1.attn.relative_position_bias_table\", \"swin_backbone.layers.1.blocks.1.attn.qkv.weight\", \"swin_backbone.layers.1.blocks.1.attn.qkv.bias\", \"swin_backbone.layers.1.blocks.1.attn.proj.weight\", \"swin_backbone.layers.1.blocks.1.attn.proj.bias\", \"swin_backbone.layers.1.blocks.1.norm2.weight\", \"swin_backbone.layers.1.blocks.1.norm2.bias\", \"swin_backbone.layers.1.blocks.1.mlp.fc1.weight\", \"swin_backbone.layers.1.blocks.1.mlp.fc1.bias\", \"swin_backbone.layers.1.blocks.1.mlp.fc2.weight\", \"swin_backbone.layers.1.blocks.1.mlp.fc2.bias\", \"swin_backbone.layers.2.downsample.norm.weight\", \"swin_backbone.layers.2.downsample.norm.bias\", \"swin_backbone.layers.2.downsample.reduction.weight\", \"swin_backbone.layers.2.blocks.0.norm1.weight\", \"swin_backbone.layers.2.blocks.0.norm1.bias\", \"swin_backbone.layers.2.blocks.0.attn.relative_position_bias_table\", \"swin_backbone.layers.2.blocks.0.attn.qkv.weight\", \"swin_backbone.layers.2.blocks.0.attn.qkv.bias\", \"swin_backbone.layers.2.blocks.0.attn.proj.weight\", \"swin_backbone.layers.2.blocks.0.attn.proj.bias\", \"swin_backbone.layers.2.blocks.0.norm2.weight\", \"swin_backbone.layers.2.blocks.0.norm2.bias\", \"swin_backbone.layers.2.blocks.0.mlp.fc1.weight\", \"swin_backbone.layers.2.blocks.0.mlp.fc1.bias\", \"swin_backbone.layers.2.blocks.0.mlp.fc2.weight\", \"swin_backbone.layers.2.blocks.0.mlp.fc2.bias\", \"swin_backbone.layers.2.blocks.1.norm1.weight\", \"swin_backbone.layers.2.blocks.1.norm1.bias\", \"swin_backbone.layers.2.blocks.1.attn.relative_position_bias_table\", \"swin_backbone.layers.2.blocks.1.attn.qkv.weight\", \"swin_backbone.layers.2.blocks.1.attn.qkv.bias\", \"swin_backbone.layers.2.blocks.1.attn.proj.weight\", \"swin_backbone.layers.2.blocks.1.attn.proj.bias\", \"swin_backbone.layers.2.blocks.1.norm2.weight\", \"swin_backbone.layers.2.blocks.1.norm2.bias\", \"swin_backbone.layers.2.blocks.1.mlp.fc1.weight\", \"swin_backbone.layers.2.blocks.1.mlp.fc1.bias\", \"swin_backbone.layers.2.blocks.1.mlp.fc2.weight\", \"swin_backbone.layers.2.blocks.1.mlp.fc2.bias\", \"swin_backbone.layers.2.blocks.2.norm1.weight\", \"swin_backbone.layers.2.blocks.2.norm1.bias\", \"swin_backbone.layers.2.blocks.2.attn.relative_position_bias_table\", \"swin_backbone.layers.2.blocks.2.attn.qkv.weight\", \"swin_backbone.layers.2.blocks.2.attn.qkv.bias\", \"swin_backbone.layers.2.blocks.2.attn.proj.weight\", \"swin_backbone.layers.2.blocks.2.attn.proj.bias\", \"swin_backbone.layers.2.blocks.2.norm2.weight\", \"swin_backbone.layers.2.blocks.2.norm2.bias\", \"swin_backbone.layers.2.blocks.2.mlp.fc1.weight\", \"swin_backbone.layers.2.blocks.2.mlp.fc1.bias\", \"swin_backbone.layers.2.blocks.2.mlp.fc2.weight\", \"swin_backbone.layers.2.blocks.2.mlp.fc2.bias\", \"swin_backbone.layers.2.blocks.3.norm1.weight\", \"swin_backbone.layers.2.blocks.3.norm1.bias\", \"swin_backbone.layers.2.blocks.3.attn.relative_position_bias_table\", \"swin_backbone.layers.2.blocks.3.attn.qkv.weight\", \"swin_backbone.layers.2.blocks.3.attn.qkv.bias\", \"swin_backbone.layers.2.blocks.3.attn.proj.weight\", \"swin_backbone.layers.2.blocks.3.attn.proj.bias\", \"swin_backbone.layers.2.blocks.3.norm2.weight\", \"swin_backbone.layers.2.blocks.3.norm2.bias\", \"swin_backbone.layers.2.blocks.3.mlp.fc1.weight\", \"swin_backbone.layers.2.blocks.3.mlp.fc1.bias\", \"swin_backbone.layers.2.blocks.3.mlp.fc2.weight\", \"swin_backbone.layers.2.blocks.3.mlp.fc2.bias\", \"swin_backbone.layers.2.blocks.4.norm1.weight\", \"swin_backbone.layers.2.blocks.4.norm1.bias\", \"swin_backbone.layers.2.blocks.4.attn.relative_position_bias_table\", \"swin_backbone.layers.2.blocks.4.attn.qkv.weight\", \"swin_backbone.layers.2.blocks.4.attn.qkv.bias\", \"swin_backbone.layers.2.blocks.4.attn.proj.weight\", \"swin_backbone.layers.2.blocks.4.attn.proj.bias\", \"swin_backbone.layers.2.blocks.4.norm2.weight\", \"swin_backbone.layers.2.blocks.4.norm2.bias\", \"swin_backbone.layers.2.blocks.4.mlp.fc1.weight\", \"swin_backbone.layers.2.blocks.4.mlp.fc1.bias\", \"swin_backbone.layers.2.blocks.4.mlp.fc2.weight\", \"swin_backbone.layers.2.blocks.4.mlp.fc2.bias\", \"swin_backbone.layers.2.blocks.5.norm1.weight\", \"swin_backbone.layers.2.blocks.5.norm1.bias\", \"swin_backbone.layers.2.blocks.5.attn.relative_position_bias_table\", \"swin_backbone.layers.2.blocks.5.attn.qkv.weight\", \"swin_backbone.layers.2.blocks.5.attn.qkv.bias\", \"swin_backbone.layers.2.blocks.5.attn.proj.weight\", \"swin_backbone.layers.2.blocks.5.attn.proj.bias\", \"swin_backbone.layers.2.blocks.5.norm2.weight\", \"swin_backbone.layers.2.blocks.5.norm2.bias\", \"swin_backbone.layers.2.blocks.5.mlp.fc1.weight\", \"swin_backbone.layers.2.blocks.5.mlp.fc1.bias\", \"swin_backbone.layers.2.blocks.5.mlp.fc2.weight\", \"swin_backbone.layers.2.blocks.5.mlp.fc2.bias\", \"swin_backbone.layers.3.downsample.norm.weight\", \"swin_backbone.layers.3.downsample.norm.bias\", \"swin_backbone.layers.3.downsample.reduction.weight\", \"swin_backbone.layers.3.blocks.0.norm1.weight\", \"swin_backbone.layers.3.blocks.0.norm1.bias\", \"swin_backbone.layers.3.blocks.0.attn.relative_position_bias_table\", \"swin_backbone.layers.3.blocks.0.attn.qkv.weight\", \"swin_backbone.layers.3.blocks.0.attn.qkv.bias\", \"swin_backbone.layers.3.blocks.0.attn.proj.weight\", \"swin_backbone.layers.3.blocks.0.attn.proj.bias\", \"swin_backbone.layers.3.blocks.0.norm2.weight\", \"swin_backbone.layers.3.blocks.0.norm2.bias\", \"swin_backbone.layers.3.blocks.0.mlp.fc1.weight\", \"swin_backbone.layers.3.blocks.0.mlp.fc1.bias\", \"swin_backbone.layers.3.blocks.0.mlp.fc2.weight\", \"swin_backbone.layers.3.blocks.0.mlp.fc2.bias\", \"swin_backbone.layers.3.blocks.1.norm1.weight\", \"swin_backbone.layers.3.blocks.1.norm1.bias\", \"swin_backbone.layers.3.blocks.1.attn.relative_position_bias_table\", \"swin_backbone.layers.3.blocks.1.attn.qkv.weight\", \"swin_backbone.layers.3.blocks.1.attn.qkv.bias\", \"swin_backbone.layers.3.blocks.1.attn.proj.weight\", \"swin_backbone.layers.3.blocks.1.attn.proj.bias\", \"swin_backbone.layers.3.blocks.1.norm2.weight\", \"swin_backbone.layers.3.blocks.1.norm2.bias\", \"swin_backbone.layers.3.blocks.1.mlp.fc1.weight\", \"swin_backbone.layers.3.blocks.1.mlp.fc1.bias\", \"swin_backbone.layers.3.blocks.1.mlp.fc2.weight\", \"swin_backbone.layers.3.blocks.1.mlp.fc2.bias\", \"swin_backbone.norm.weight\", \"swin_backbone.norm.bias\", \"cbam.channel_att.fc.0.weight\", \"cbam.channel_att.fc.2.weight\", \"cbam.spatial_att.conv.weight\", \"classifier.4.weight\", \"classifier.4.bias\", \"classifier.5.weight\", \"classifier.5.bias\", \"classifier.5.running_mean\", \"classifier.5.running_var\", \"classifier.5.num_batches_tracked\", \"classifier.8.weight\", \"classifier.8.bias\", \"classifier.1.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\". \n",
            "\n",
            " Backing up to: /content/drive/MyDrive/GCON_FINAL_SUBMISSION_20260126_141518\n",
            "\tzip warning: name not matched: drive/.Encrypted/.shortcut-targets-by-id/11BV-jSV9J8kMxY32Oyo0uJzRGuaak7CU/Chemistry Group\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP_NEW.ipynb (deflated 48%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Boston Housing Price Prediction.ipynb (deflated 35%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Sales Analysis.ipynb (deflated 41%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/lab4.ipynb (deflated 30%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Day 1.ipynb (deflated 28%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/pbl new.ipynb (deflated 43%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL.ipynb (deflated 32%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL2.ipynb (deflated 71%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Pblmk2.ipynb (deflated 79%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL test 2.ipynb (deflated 80%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Pbl test.ipynb (deflated 67%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL test  .ipynb (deflated 41%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled0.ipynb (deflated 30%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_test_.ipynb (deflated 44%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled1.ipynb (deflated 38%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Dataset test.ipynb (deflated 67%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/pbl dataset test.ipynb (deflated 41%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Combining_Dataset.ipynb (deflated 62%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Datasets Combined.ipynb (deflated 63%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Dataset_Merged.ipynb (deflated 94%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_Experiment.ipynb (deflated 31%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled2.ipynb (deflated 30%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/no.ipynb (deflated 92%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled3.ipynb (deflated 31%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL(EXP-5).ipynb (deflated 38%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL(FINAL).ipynb (deflated 60%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/merged_notebook (1).ipynb (deflated 30%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_notebook.ipynb (deflated 37%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled4.ipynb (deflated 50%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/merged_notebook.ipynb (deflated 32%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_MAIN_notebook.ipynb (deflated 31%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Pbl_Re-evaluation.ipynb (deflated 67%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled5.ipynb (deflated 31%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Pbl_Re_evaluation.ipynb (deflated 72%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP-1&3_FINAL.ipynb (deflated 40%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP-2_FINAL.ipynb (deflated 43%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP-4&5_FINAL.ipynb (deflated 40%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_Master_Notebook.ipynb.ipynb (deflated 38%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_Master_Notebook (2).ipynb (deflated 39%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_Master_Notebook (1).ipynb (deflated 39%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/PBL_Master_Notebook.ipynb (deflated 38%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Denoising.ipynb (deflated 27%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Augmentation+Skull_Stripping.ipynb (deflated 28%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Dataset_Prep.ipynb (deflated 92%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/DATA_PREP.ipynb (deflated 90%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/dataset(Figshare).ipynb (deflated 92%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Kaggle(Figshare).ipynb (deflated 52%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/FIGSHARE(DATASET_FINAL) (2).ipynb (deflated 75%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/CLAHE_SKULL_STRIPPING_&_DATA_AUGMENTATION.ipynb (deflated 26%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/MASTER_COMPARITIVE_ABLATION_NOTEBOOK.ipynb (deflated 26%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/FIGSHARE(DATASET_FINAL) (1).ipynb (deflated 80%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/Untitled6.ipynb (deflated 40%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/FIGSHARE(DATASET_FINAL).ipynb (deflated 80%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP-1.ipynb (deflated 31%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP-2_Transformers.ipynb (deflated 49%)\n",
            "  adding: drive/MyDrive/Colab Notebooks/EXP_ImageLevel_AllModels_Figshare.ipynb (deflated 92%)\n",
            "  adding: drive/MyDrive/experiment_outputs/weights/swin_tiny_patch4_window7_224.pth (deflated 7%)\n",
            "  adding: drive/MyDrive/experiment_outputs/weights/vit_small_patch16_224.pth (deflated 7%)\n",
            "  adding: drive/MyDrive/experiment_outputs/Figshare_ImageLevel/weights/ResNet50_ImageSplit.pth (deflated 7%)\n",
            "  adding: drive/MyDrive/experiment_outputs/Figshare_ImageLevel/weights/DenseNet121_ImageSplit.pth (deflated 8%)\n",
            "  adding: drive/MyDrive/experiment_outputs/Figshare_ImageLevel/weights/EfficientNetB0_ImageSplit.pth (deflated 8%)\n",
            "  adding: drive/MyDrive/experiment_outputs/Figshare_ImageLevel/weights/ViT_Small_ImageSplit.pth (deflated 7%)\n",
            "  adding: drive/MyDrive/experiment_outputs/Figshare_ImageLevel/weights/Swin_Tiny_ImageSplit.pth\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# =============================================================================\n",
        "# REDEFINE THE MODEL CLASS (To fix the Attribute Error)\n",
        "# =============================================================================\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
        "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
        "        out = self.sigmoid(avg_out + max_out).view(b, c, 1, 1)\n",
        "        return x * out\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        out = torch.cat([avg_out, max_out], dim=1)\n",
        "        out = self.sigmoid(self.conv(out))\n",
        "        return x * out\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(channels)\n",
        "        self.sa = SpatialAttention()\n",
        "    def forward(self, x):\n",
        "        x = self.ca(x)\n",
        "        x = self.sa(x)\n",
        "        return x\n",
        "\n",
        "class AttentionEnhancedSwin(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        # FORCE name to 'swin' to match the GradCAM code\n",
        "        self.swin = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=0)\n",
        "        self.feature_dim = self.swin.num_features\n",
        "        self.cbam = CBAM(self.feature_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        features = self.swin(x)\n",
        "        features_4d = features.unsqueeze(-1).unsqueeze(-1)\n",
        "        features_attn = self.cbam(features_4d)\n",
        "        features_attn = features_attn.squeeze(-1).squeeze(-1)\n",
        "        return self.classifier(features_attn)\n",
        "\n",
        "# =============================================================================\n",
        "# SMART LOAD & GRAD-CAM\n",
        "# =============================================================================\n",
        "print(\" Locating model file...\")\n",
        "# Find ANY file ending in .pth that looks like our model\n",
        "possible_files = glob.glob('**/*Attention*best.pth', recursive=True)\n",
        "\n",
        "if not possible_files:\n",
        "    print(\" Could not find 'Attention' model. Trying ANY .pth file...\")\n",
        "    possible_files = glob.glob('**/*.pth', recursive=True)\n",
        "\n",
        "if possible_files:\n",
        "    weights_path = possible_files[0] # Pick the first one found\n",
        "    print(f\" Found weights: {weights_path}\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = AttentionEnhancedSwin(num_classes=3).to(device)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "        print(\" Weights loaded successfully.\")\n",
        "\n",
        "        # --- Run GradCAM ---\n",
        "        # Target the last LayerNorm of the Swin backbone\n",
        "        target_layers = [model.swin.norm]\n",
        "        cam = GradCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "        # Find images\n",
        "        test_images = glob.glob('**/*.jpg', recursive=True)\n",
        "        if len(test_images) > 0:\n",
        "            # Pick a random image\n",
        "            img_path = np.random.choice(test_images)\n",
        "\n",
        "            rgb_img = cv2.imread(img_path, 1)[:, :, ::-1]\n",
        "            rgb_img = cv2.resize(rgb_img, (224, 224))\n",
        "            rgb_img_float = np.float32(rgb_img) / 255\n",
        "            input_tensor = preprocess_image(rgb_img_float, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "            grayscale_cam = cam(input_tensor=input_tensor.to(device))[0, :]\n",
        "            visualization = show_cam_on_image(rgb_img_float, grayscale_cam, use_rgb=True)\n",
        "\n",
        "            # Save\n",
        "            cv2.imwrite(\"Final_GradCAM.png\", cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.subplot(1,2,1); plt.imshow(rgb_img); plt.title(\"Original\")\n",
        "            plt.subplot(1,2,2); plt.imshow(visualization); plt.title(\"Attention Map\")\n",
        "            plt.show()\n",
        "            print(\" Grad-CAM generated and saved as 'Final_GradCAM.png'\")\n",
        "        else:\n",
        "            print(\" No jpg images found to visualize.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error during visualization (skipping): {e}\")\n",
        "else:\n",
        "    print(\" Critical: No .pth model file found. Cannot generate GradCAM.\")\n",
        "\n",
        "# =============================================================================\n",
        "# BACKUP EVERYTHING\n",
        "# =============================================================================\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "dest_dir = f\"/content/drive/MyDrive/GCON_FINAL_SUBMISSION_{timestamp}\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "print(f\"\\n Backing up to: {dest_dir}\")\n",
        "!cp *.pth \"{dest_dir}/\" 2>/dev/null\n",
        "!cp *.png \"{dest_dir}/\" 2>/dev/null\n",
        "!cp *.csv \"{dest_dir}/\" 2>/dev/null\n",
        "!zip -r \"{dest_dir}/PROJECT_CODE_DATA.zip\" . -i *.py *.ipynb *.pth *.csv *.png\n",
        "\n",
        "print(f\" DONE. You can close the tab.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "NUM_CLASSES = 3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "save_dir = \"/content/drive/MyDrive/GCON_FINAL_COMPARISON_RUN_V2\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Standard ImageNet Normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def train_model_3split(model_name, split_name, dataset_path):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" PROCESSING: {model_name} on {split_name} SPLIT\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # LOCATE FOLDERS (Handles your 3-folder structure)\n",
        "    # We look for the folder that contains 'Training', 'Validation', 'Testing'\n",
        "    target_root = \"\"\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        if \"Training\" in dirs and \"Validation\" in dirs:\n",
        "            target_root = root\n",
        "            break\n",
        "\n",
        "    if not target_root:\n",
        "        # Fallback: Assume the path provided is the root\n",
        "        target_root = dataset_path\n",
        "\n",
        "    train_dir = os.path.join(target_root, \"Training\")\n",
        "    val_dir   = os.path.join(target_root, \"Validation\")\n",
        "    test_dir  = os.path.join(target_root, \"Testing\")\n",
        "\n",
        "    # VERIFY PATHS\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\" ERROR: Could not find Training folder at {train_dir}\")\n",
        "        return 0.0\n",
        "\n",
        "    print(f\" Loading Data from: {target_root}\")\n",
        "\n",
        "    # LOAD DATA\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "    val_data   = datasets.ImageFolder(val_dir, transform=transform)\n",
        "    test_data  = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\" Stats: {len(train_data)} Train | {len(val_data)} Val | {len(test_data)} Test\")\n",
        "\n",
        "    # LOAD MODEL\n",
        "\n",
        "    model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=NUM_CLASSES)\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # TRAINING LOOP\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # 1. TRAIN\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 2. VALIDATION\n",
        "        model.eval()\n",
        "        correct = 0; total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        print(f\"   Epoch {epoch+1}: Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/{model_name}_{split_name}_best.pth\")\n",
        "\n",
        "    # 3. FINAL TEST\n",
        "    print(f\" Loading Best Model for FINAL TESTING...\")\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/{model_name}_{split_name}_best.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    final_test_acc = 100 * correct / total\n",
        "    print(f\" FINAL TEST ACCURACY ({model_name}): {final_test_acc:.2f}%\")\n",
        "    return final_test_acc\n",
        "\n",
        "# --- RUN EXPERIMENT A: NORMAL SPLIT ---\n",
        "swin_normal = train_model_3split(\"Swin-Tiny\", \"NORMAL\", \"/content/dataset_normal\")\n",
        "\n",
        "# --- RUN EXPERIMENT B: PATIENT SPLIT ---\n",
        "swin_patient = train_model_3split(\"Swin-Tiny\", \"PATIENT\", \"/content/dataset_patient\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl-Q1X7t05qw",
        "outputId": "4e6bd578-423e-4762-a790-ab35fe1e99bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " PROCESSING: Swin-Tiny on NORMAL SPLIT\n",
            "============================================================\n",
            " Loading Data from: /content/dataset_normal/Figshare_ImageLevel\n",
            " Stats: 2144 Train | 459 Val | 461 Test\n",
            "   Epoch 1: Val Acc: 91.94%\n",
            "   Epoch 2: Val Acc: 97.39%\n",
            "   Epoch 3: Val Acc: 97.60%\n",
            "   Epoch 4: Val Acc: 96.51%\n",
            "   Epoch 5: Val Acc: 97.39%\n",
            "   Epoch 6: Val Acc: 98.04%\n",
            "   Epoch 7: Val Acc: 96.95%\n",
            "   Epoch 8: Val Acc: 98.04%\n",
            "   Epoch 9: Val Acc: 95.86%\n",
            "   Epoch 10: Val Acc: 96.51%\n",
            "🏁 Loading Best Model for FINAL TESTING...\n",
            " FINAL TEST ACCURACY (Swin-Tiny): 97.83%\n",
            "\n",
            "============================================================\n",
            " PROCESSING: Swin-Tiny on PATIENT SPLIT\n",
            "============================================================\n",
            " Loading Data from: /content/dataset_patient/Figshare_Dataset\n",
            " Stats: 2159 Train | 433 Val | 472 Test\n",
            "   Epoch 1: Val Acc: 92.15%\n",
            "   Epoch 2: Val Acc: 94.46%\n",
            "   Epoch 3: Val Acc: 94.69%\n",
            "   Epoch 4: Val Acc: 95.84%\n",
            "   Epoch 5: Val Acc: 95.61%\n",
            "   Epoch 6: Val Acc: 90.99%\n",
            "   Epoch 7: Val Acc: 94.00%\n",
            "   Epoch 8: Val Acc: 95.38%\n",
            "   Epoch 9: Val Acc: 94.23%\n",
            "   Epoch 10: Val Acc: 95.38%\n",
            "🏁 Loading Best Model for FINAL TESTING...\n",
            " FINAL TEST ACCURACY (Swin-Tiny): 96.19%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "NUM_CLASSES = 3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "save_dir = \"/content/drive/MyDrive/GCON_FINAL_COMPARISON_RUN_V2\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def train_vit_patient(dataset_path):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🚀 TRAINING: ViT-Small (Standard) on PATIENT SPLIT\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Locate Folders\n",
        "    target_root = \"\"\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        if \"Training\" in dirs and \"Validation\" in dirs:\n",
        "            target_root = root\n",
        "            break\n",
        "    if not target_root: target_root = dataset_path\n",
        "\n",
        "    train_dir = os.path.join(target_root, \"Training\")\n",
        "    val_dir   = os.path.join(target_root, \"Validation\")\n",
        "    test_dir  = os.path.join(target_root, \"Testing\")\n",
        "\n",
        "    # Load Data\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "    val_data   = datasets.ImageFolder(val_dir, transform=transform)\n",
        "    test_data  = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\" Stats: {len(train_data)} Train | {len(val_data)} Val | {len(test_data)} Test\")\n",
        "\n",
        "    # Load ViT Model\n",
        "    # We use the standard 'vit_small_patch16_224'\n",
        "    model = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=NUM_CLASSES)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Standard AdamW\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Train Loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0; total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        print(f\"   Epoch {epoch+1}: Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/ViT_Small_PATIENT_best.pth\")\n",
        "\n",
        "    # Final Test\n",
        "    print(f\"🏁 Final Testing (Best ViT Model)...\")\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/ViT_Small_PATIENT_best.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    final_acc = 100 * correct / total\n",
        "    print(f\"FINAL ViT ACCURACY (Patient Split): {final_acc:.2f}%\")\n",
        "    return final_acc\n",
        "\n",
        "# RUN IT\n",
        "vit_score = train_vit_patient(\"/content/dataset_patient\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEJlrF9o2EKg",
        "outputId": "46f04bbd-ed14-41ee-a4ab-8dd85a03e0dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🚀 TRAINING: ViT-Small (Standard) on PATIENT SPLIT\n",
            "============================================================\n",
            " Stats: 2159 Train | 433 Val | 472 Test\n",
            "   Epoch 1: Val Acc: 89.38%\n",
            "   Epoch 2: Val Acc: 92.61%\n",
            "   Epoch 3: Val Acc: 95.84%\n",
            "   Epoch 4: Val Acc: 93.07%\n",
            "   Epoch 5: Val Acc: 94.69%\n",
            "   Epoch 6: Val Acc: 92.38%\n",
            "   Epoch 7: Val Acc: 90.76%\n",
            "   Epoch 8: Val Acc: 94.46%\n",
            "   Epoch 9: Val Acc: 89.84%\n",
            "   Epoch 10: Val Acc: 95.15%\n",
            "🏁 Final Testing (Best ViT Model)...\n",
            "FINAL ViT ACCURACY (Patient Split): 94.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RUN ViT (Standard) on NORMAL SPLIT ---\n",
        "# This gives you the \"Baseline\" score to compare against the \"Hard\" score.\n",
        "\n",
        "# Re-using the same function, just changing the path and save name.\n",
        "def train_vit_normal(dataset_path):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" TRAINING: ViT-Small (Standard) on NORMAL SPLIT\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Locate Folders\n",
        "    target_root = \"\"\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        if \"Training\" in dirs and \"Validation\" in dirs:\n",
        "            target_root = root\n",
        "            break\n",
        "    if not target_root: target_root = dataset_path\n",
        "\n",
        "    train_dir = os.path.join(target_root, \"Training\")\n",
        "    val_dir   = os.path.join(target_root, \"Validation\")\n",
        "    test_dir  = os.path.join(target_root, \"Testing\")\n",
        "\n",
        "    # Load Data (Standard Transform)\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "    val_data   = datasets.ImageFolder(val_dir, transform=transform)\n",
        "    test_data  = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\" Stats: {len(train_data)} Train | {len(val_data)} Val | {len(test_data)} Test\")\n",
        "\n",
        "    # Load Model\n",
        "    model = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=NUM_CLASSES)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0; total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        print(f\"   Epoch {epoch+1}: Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"{save_dir}/ViT_Small_NORMAL_best.pth\")\n",
        "\n",
        "    # Final Test\n",
        "    print(f\"🏁 Final Testing (Normal Split)...\")\n",
        "    model.load_state_dict(torch.load(f\"{save_dir}/ViT_Small_NORMAL_best.pth\"))\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    final_acc = 100 * correct / total\n",
        "    print(f\" FINAL ViT ACCURACY (Normal Split): {final_acc:.2f}%\")\n",
        "    return final_acc\n",
        "\n",
        "# RUN IT\n",
        "vit_normal_score = train_vit_normal(\"/content/dataset_normal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTQoQ3RA2Ej4",
        "outputId": "09c16e06-81a7-4830-b8d0-20c9f19f6d39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " TRAINING: ViT-Small (Standard) on NORMAL SPLIT\n",
            "============================================================\n",
            " Stats: 2144 Train | 459 Val | 461 Test\n",
            "   Epoch 1: Val Acc: 84.75%\n",
            "   Epoch 2: Val Acc: 96.08%\n",
            "   Epoch 3: Val Acc: 96.73%\n",
            "   Epoch 4: Val Acc: 96.95%\n",
            "   Epoch 5: Val Acc: 96.95%\n",
            "   Epoch 6: Val Acc: 95.86%\n",
            "   Epoch 7: Val Acc: 96.73%\n",
            "   Epoch 8: Val Acc: 95.42%\n",
            "   Epoch 9: Val Acc: 93.03%\n",
            "   Epoch 10: Val Acc: 91.94%\n",
            "🏁 Final Testing (Normal Split)...\n",
            " FINAL ViT ACCURACY (Normal Split): 97.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l-FsL0zZdkR1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
